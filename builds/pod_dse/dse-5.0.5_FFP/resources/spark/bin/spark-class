#!/usr/bin/env bash

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

if [ -z "${SPARK_HOME}" ]; then
  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
fi

. "${SPARK_HOME}"/bin/load-spark-env.sh

# Find the java binary
if [ -n "${JAVA_HOME}" ]; then
  RUNNER="${JAVA_HOME}/bin/java"
else
  if [ `command -v java` ]; then
    RUNNER="java"
  else
    echo "JAVA_HOME is not set" >&2
    exit 1
  fi
fi

# DSE changes start
# DSE doesn't use Spark assembly, so this part is commented out
if [ "$DSE_MODE" != "1" ]; then
# Find assembly jar
SPARK_ASSEMBLY_JAR=
if [ -f "${SPARK_HOME}/RELEASE" ]; then
  ASSEMBLY_DIR="${SPARK_HOME}/lib"
else
  ASSEMBLY_DIR="${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION"
fi

GREP_OPTIONS=
num_jars="$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" | wc -l)"
if [ "$num_jars" -eq "0" -a -z "$SPARK_ASSEMBLY_JAR" -a "$SPARK_PREPEND_CLASSES" != "1" ]; then
  echo "Failed to find Spark assembly in $ASSEMBLY_DIR." 1>&2
  echo "You need to build Spark before running this program." 1>&2
  exit 1
fi
if [ -d "$ASSEMBLY_DIR" ]; then
  ASSEMBLY_JARS="$(ls -1 "$ASSEMBLY_DIR" | grep "^spark-assembly.*hadoop.*\.jar$" || true)"
  if [ "$num_jars" -gt "1" ]; then
    echo "Found multiple Spark assembly jars in $ASSEMBLY_DIR:" 1>&2
    echo "$ASSEMBLY_JARS" 1>&2
    echo "Please remove all but one jar." 1>&2
    exit 1
  fi
fi

SPARK_ASSEMBLY_JAR="${ASSEMBLY_DIR}/${ASSEMBLY_JARS}"

LAUNCH_CLASSPATH="$SPARK_ASSEMBLY_JAR"

# Add the launcher build dir to the classpath if requested.
if [ -n "$SPARK_PREPEND_CLASSES" ]; then
  LAUNCH_CLASSPATH="${SPARK_HOME}/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"
fi

export _SPARK_ASSEMBLY="$SPARK_ASSEMBLY_JAR"
fi

# DSE changes end

# DSE changes start
# For DSE we need to define the class path in a different way
if [ "$DSE_MODE" == "1" ]; then
  export _SPARK_ASSEMBLY="$("$DSE_SCRIPT" spark-classpath)"
  LAUNCH_CLASSPATH="$(find "$SPARK_HOME"/lib -name 'spark-launcher_*.jar')"
fi
# DSE changes end


# DSE changes start
# This method is used by spark-class script in order to replace the original Spark classes by our custom
# implementations. We need to apply this method right after Spark launcher generates the command line.
# This is needed because Spark launcher take the class name as an argument and basing on it, Spark
# launcher does specific tasks. Therefore, if we want Spark launcher to generate the same command line
# for our implementation, we need to provide the original class names to the launcher and then replace
# them with our implementations, before the generated command is executed.
# params  : original command line argumetns array
exec_with_dse_impl() {
    NEW_ARGS=()
    for ARG in "$@"
    do
        if [ "$ARG" == "org.apache.spark.repl.Main" ]; then
            ARG="com.datastax.bdp.spark.SparkReplMain"
        elif [ "$ARG" == "org.apache.spark.deploy.SparkSubmit" ]; then
            ARG="org.apache.spark.deploy.DseSparkSubmitBootstrapper"
        fi

        NEW_ARGS+=("$ARG")
    done

    exec "${NEW_ARGS[@]}"
}
# DSE changes end

# For tests
if [[ -n "$SPARK_TESTING" ]]; then
  unset YARN_CONF_DIR
  unset HADOOP_CONF_DIR
fi

# The launcher library will print arguments separated by a NULL character, to allow arguments with
# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating
# an array that will be used to exec the final command.
CMD=()
while IFS= read -d '' -r ARG; do
  CMD+=("$ARG")
done < <("$RUNNER" -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@")

# DSE changes start
if [ "$DSE_MODE" == "1" ]; then
  exec_with_dse_impl "${CMD[@]}"
else
  exec "${CMD[@]}"
fi
# DSE changes end
