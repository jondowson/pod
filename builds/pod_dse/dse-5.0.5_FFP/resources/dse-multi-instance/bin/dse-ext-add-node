#!/bin/sh
#
#Copyright DataStax, Inc.
#Please see the included license file for details
#

#TITLE:Add new node to the DSE multi-instance server.

if [ -z "$BASH_VERSION" ]; then
    exec bash "$0" "$@"
    exit 1  # Will only get here if exec itself fails to run
fi

set -f

# This script adds a DSE nodes to a DSE multi-instance server. This script will:
# - Install and configure a node in a DSE multi-instance server
# - Create init.d scripts for nodes
# - Update the runlevels

# Usage: sudo dse add-node --node-id=ID

# For example, adding node3 to the server:
# sudo dse add-node --node-id=node3

###
# You shouldn't need to change anything below here
###

show_help()
{
    echo "Usage: $0 --node-id=ID [--cluster=NAME] [--jmxport=port] [--listen-address=IP] [--rpc-address=IP] [--cpus=N] [--max-heap-size=SIZE] [--data-directory=DIRECTORY] [--hints-directory=DIRECTORY] [--commit-directory=DIRECTORY] [--saved-caches-directory=DIRECTORY] [--logs-directory=DIRECTORY] [--spark-worker-directory=DIRECTORY] [--spark-worker-cores=CORES] [--spark-worker-memory=MEMORY] [--unix-username=NAME] [--unix-group=GROUP] [--tomcat-logs=DIRECTORY] [--hadoop-logs=DIRECTORY] [--hive-logs=DIRECTORY] [--pig-logs=DIRECTORY] [--mahout-logs=DIRECTORY] [--rack=RACK --dc=DC] [--seeds=IP1,IP2,...] [--hadoop-tt-cores=CORES] [--hadoop-tt-memory=MEMORY] [--search] [--analytics] [--graph] [--hadoop] [--cfs] [--num-tokens=N] [--spark-shuffle-service-port=PORT] [--dsefs]"
    echo "Where:"
    echo "node-id: is the ID for the node. It's an alphanumeric value, e.g. 'node1'"
    echo "cluster: is the name of the cluster the node belongs to. Only non-whitespace values are supported."
    echo "jmxport: is the JMX port for the new node."
    echo "listen-address: is the address Cassandra will bind to."
    echo "rpc-address: is the address Cassandra will bind to for rpc requests."
    echo "cpus: is the number of cores Cassandra will use."
    echo "max-heap-size: is the amount of memory used (Xmx setting) for the heap by the JVM, e.g. --max-heap-size=8G. If no size units are specified, M is assumed, i.e. --max-heap-size=800 is equivalent to --max-heap-size=800M"
    echo "data-directory: is the root directory for all the data for the node. Defaults to /var/lib/NODEID/data"
    echo "hints-directory: is the hints directory for the node. Defaults to /var/lib/NODEID/hints"
    echo "commit-directory: is the commit log directory for the node. Defaults to /var/lib/NODEID/commitlog"
    echo "saved-caches-directory: is the saved caches directory for the node. Defaults to /var/lib/NODEID/saved_caches"
    echo "logs-directory: is the root directory for all the logs for the node. Defaults to /var/log/NODEID"
    echo "tomcat-logs: is the log directory for tomcat logs. Defaults to logs-directory/tomcat"
    echo "hadoop-logs: is the log directory for hadoop logs. Defaults to logs-directory/hadoop"
    echo "hive-logs: is the log directory for hive logs. Defaults to logs-directory/hive"
    echo "pig-logs: is the log directory for pig logs. Defaults to logs-directory/pig"
    echo "mahout-logs: is the log directory for mahout logs. Defaults to logs-directory/mahout"
    echo "spark-worker-directory: is the data directory for spark worker for the node. Defaults to /var/lib/NODEID/spark/worker"
    echo "spark-worker-cores: the maximum number of cores used by Spark executors"
    echo "spark-worker-memory: maximum amount of memory used by Spark executors, You can use typical suffixes for memory sizes, e.g. k - kilo, m - mega, g - giga"
    echo "spark-local-directory: is the local directory for spark worker for the node. Defaults to /var/lib/NODEID/spark/rdd"
    echo "spark-log-directory: is the log directory for the spark worker for the node. Defaults to /var/log/NODEID/spark/worker"
    echo "unix-username: is the unix username that will own the node configuration. Defaults to 'cassandra'"
    echo "unix-group: is the unix group that will own the node configuration. Defaults to 'cassandra'"
    echo "rack: is the rack placement for the node. The value will be checked against that of the default node (dse), and if it doesn't match a warning will be issued and the default rack left unchanged."
    echo "dc: is the data center placement for the node. The value will be checked against that of the default node (dse), and if it doesn't match a warning will be issued and the default DC left unchanged."
    echo "seeds: comma separated list of IP addresses to be used as seeds"
    echo "hadoop-tt-cores: maximum number of slots that can be allocated by TaskTracker for running user tasks"
    echo "hadoop-tt-memory: maximum amount of memory that can be allocated by TaskTracker for running user tasks. You can use typical suffixes for memory sizes, e.g. k - kilo, m - mega, g - giga"
    echo "search: enable DSE Search on the new node"
    echo "analytics: enable DSE Analytics (Spark) on the new node"
    echo "hadoop: enable DSE Analytics (Hadoop) on the new node"
    echo "graph: enable DSE Graph on the new node"
    echo "cfs: enable CFS without Spark or Hadoop on the new node"
    echo "num-tokens: set the number of tokens assigned to the node."
    echo "spark-shuffle-service-port: set the Spark shuffle service port"
    echo "dsefs: enable DSEFS on the new node"
    echo
    echo "When an optional parameter is absent, the default value will be left unchanged."
}

while :; do
    case $1 in
        -h|-\?|--help)
            show_help
            exit
            ;;
        -n|--node-id)
            if [ -n "$2" ]; then
                NODEID=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--node-id" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --node-id=?*)
            NODEID=${1#*=}
            ;;
        --node-id=)
        printf 'ERROR: "--node-id" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --cluster)
            if [ -n "$2" ]; then
                CLUSTER=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--cluster" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --cluster=?*)
            CLUSTER=${1#*=}
            ;;
        --cluster=)
        printf 'ERROR: "--cluster" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -j|--jmxport)
            if [ -n "$2" ]; then
                JMX_PORT=$2
                shift 2
                continue
            else
                printf 'ERROR: "--jmxport" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --jmxport=?*)
            JMX_PORT=${1#*=}
            ;;
        --jmxport=)         # Handle the case of an empty --jmxport=
        printf 'ERROR: "--jmxport" requires a non-empty option argument.\n' >&2
        exit 1
        ;;
        -L|--listen-address)
            if [ -n "$2" ]; then
                LISTEN_ADDRESS=$2
                shift 2
                continue
            else
                printf 'ERROR: "--listen-address" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --listen-address=?*)
            LISTEN_ADDRESS=${1#*=}
            ;;
        --listen-address=)         # Handle the case of an empty --jmxport=
        printf 'ERROR: "--listen-address" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -r|--rpc-address)
            if [ -n "$2" ]; then
                RPC_ADDRESS=$2
                shift 2
                continue
            else
                printf 'ERROR: "--rpc-address" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --rpc-address=?*)
            RPC_ADDRESS=${1#*=}
            ;;
        --rpc-address=)         # Handle the case of an empty --jmxport=
        printf 'ERROR: "--rpc-address" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -C|--cpus)
            if [ -n "$2" ]; then
                CPUS=$2
                shift 2
                continue
            else
                printf 'ERROR: "--cpus" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --cpus=?*)
            CPUS=${1#*=}
            ;;
        --cpus=)         # Handle the case of an empty --cpus=
        printf 'ERROR: "--cpus" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -m|--max-heap-size)
            if [ -n "$2" ]; then
                MAX_HEAP_SIZE=$2
                shift 2
                continue
            else
                printf 'ERROR: "--max-heap-size" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --max-heap-size=?*)
            MAX_HEAP_SIZE=${1#*=}
            ;;
        --max-heap-size=)         # Handle the case of an empty --max-heap-size=
        printf 'ERROR: "--max-heap-size" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -b|--base-directory)
            if [ -n "$2" ]; then
                BASE_DIRECTORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--base-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --base-directory=?*)
            BASE_DIRECTORY=${1#*=}
            ;;
        --base-directory=)
        printf 'ERROR: "--base-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -d|--data-directory)
            if [ -n "$2" ]; then
                DATA_DIRECTORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--data-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --data-directory=?*)
            DATA_DIRECTORY=${1#*=}
            ;;
        --data-directory=)
        printf 'ERROR: "--data-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -i|---hints-directory)
            if [ -n "$2" ]; then
                HINTS_DIRECTORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--hints-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --hints-directory=?*)
            HINTS_DIRECTORY=${1#*=}
            ;;
        --hints-directory=)
        printf 'ERROR: "--hints-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -c|--commit-directory)
            if [ -n "$2" ]; then
                COMMIT_DIRECTORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--commit-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --commit-directory=?*)
            COMMIT_DIRECTORY=${1#*=}
            ;;
        --commit-directory=)
        printf 'ERROR: "--commit-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -s|--saved-caches-directory)
            if [ -n "$2" ]; then
                CACHES_DIRECTORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--saved-caches-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --saved-caches-directory=?*)
            CACHES_DIRECTORY=${1#*=}
            ;;
        --saved-caches-directory=)
        printf 'ERROR: "--saved-caches-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -l|--logs-directory)
            if [ -n "$2" ]; then
                ROOT_LOGS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--logs-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --logs-directory=?*)
            ROOT_LOGS=${1#*=}
            ;;
        --logs-directory=)
        printf 'ERROR: "--logs-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --tomcat-logs)
            if [ -n "$2" ]; then
                TOMCAT_LOGS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--tomcat-logs" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --tomcat-logs=?*)
            TOMCAT_LOGS=${1#*=}
            ;;
        --tomcat-logs=)
        printf 'ERROR: "--tomcat-logs" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --hadoop-logs)
            if [ -n "$2" ]; then
                HADOOP_LOGS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--hadoop-logs" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --hadoop-logs=?*)
            HADOOP_LOGS=${1#*=}
            ;;
        --hadoop-logs=)
        printf 'ERROR: "--hadoop-logs" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --hadoop-tt-cores)
            if [ -n "$2" ]; then
                HADOOP_TT_CORES=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--hadoop-tt-cores" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --hadoop-tt-cores=?*)
            HADOOP_TT_CORES=${1#*=}
            ;;
        --hadoop-tt-cores=)
        printf 'ERROR: "--hadoop-tt-cores" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --hadoop-tt-memory)
            if [ -n "$2" ]; then
                HADOOP_TT_MEMORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--hadoop-tt-memory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --hadoop-tt-memory=?*)
            HADOOP_TT_MEMORY=${1#*=}
            ;;
        --hadoop-tt-memory=)
        printf 'ERROR: "--hadoop-tt-memory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --hive-logs)
            if [ -n "$2" ]; then
                HIVE_LOG_ROOT=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--hive-logs" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --hive-logs=?*)
            HIVE_LOG_ROOT=${1#*=}
            ;;
        --hive-logs=)
        printf 'ERROR: "--hive-logs" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --pig-logs)
            if [ -n "$2" ]; then
                PIG_LOG_ROOT=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--pig-logs" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --pig-logs=?*)
            PIG_LOG_ROOT=${1#*=}
            ;;
        --pig-logs=)
        printf 'ERROR: "--pig-logs" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --mahout-logs)
            if [ -n "$2" ]; then
                MAHOUT_LOGS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--mahout-logs" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --mahout-logs=?*)
            MAHOUT_LOGS=${1#*=}
            ;;
        --mahout-logs=)
        printf 'ERROR: "--mahout-logs" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -w|--spark-worker-directory)
            if [ -n "$2" ]; then
                SPARK_WORKER_DIR=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-worker-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-worker-directory=?*)
            SPARK_WORKER_DIR=${1#*=}
            ;;
        --spark-worker-directory=)
        printf 'ERROR: "--spark-worker-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -w|--spark-worker-cores)
            if [ -n "$2" ]; then
                SPARK_WORKER_CORES=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-worker-cores" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-worker-cores=?*)
            SPARK_WORKER_CORES=${1#*=}
            ;;
        --spark-worker-cores=)
        printf 'ERROR: "--spark-worker-cores" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -w|--spark-worker-memory)
            if [ -n "$2" ]; then
                SPARK_WORKER_MEMORY=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-worker-memory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-worker-memory=?*)
            SPARK_WORKER_MEMORY=${1#*=}
            ;;
        --spark-worker-memory=)
        printf 'ERROR: "--spark-worker-memory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -k|--spark-local-directory)
            if [ -n "$2" ]; then
                SPARK_LOCAL_DIRS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-local-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-local-directory=?*)
            SPARK_LOCAL_DIRS=${1#*=}
            ;;
        --spark-local-directory=)
        printf 'ERROR: "--spark-local-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -p|--spark-log-directory)
            if [ -n "$2" ]; then
                SPARK_WORKER_LOG_DIR=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-log-directory" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-log-directory=?*)
            SPARK_WORKER_LOG_DIR=${1#*=}
            ;;
        --spark-log-directory=)
        printf 'ERROR: "--spark-log-directory" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -u|--unix-username)
            if [ -n "$2" ]; then
                UNIX_USERNAME=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--unix-username" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --unix-username=?*)
            UNIX_USERNAME=${1#*=}
            ;;
        --unix-username=)
        printf 'ERROR: "--unix-username" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        -o|--unix-group)
            if [ -n "$2" ]; then
                UNIX_GROUP=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--unix-group" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --unix-group=?*)
            UNIX_GROUP=${1#*=}
            ;;
        --unix-group=)
        printf 'ERROR: "--unix-group" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --seeds)
            if [ -n "$2" ]; then
                SEEDS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--seeds" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --seeds=?*)
            SEEDS=${1#*=}
            ;;
        --seeds=)
        printf 'ERROR: "--seeds" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --rack)
            if [ -n "$2" ]; then
                RACK=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--rack" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --rack=?*)
            RACK=${1#*=}
            ;;
        --rack=)
        printf 'ERROR: "--rack" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --dc)
            if [ -n "$2" ]; then
                DC=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--dc" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --dc=?*)
            DC=${1#*=}
            ;;
        --dc=)
        printf 'ERROR: "--dc" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --search)
            SOLR_ENABLED=1
            shift;
            continue
            ;;

        --analytics)
            SPARK_ENABLED=1
            shift;
            continue
            ;;

        --graph)
            GRAPH_ENABLED=1
            shift;
            continue
            ;;

        --hadoop)
            HADOOP_ENABLED=1
            shift;
            continue
            ;;

        --cfs)
            CFS_ENABLED=1
            shift;
            continue
            ;;

        --num-tokens)
            if [ -n "$2" ]; then
                NUM_TOKENS=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--num-tokens" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --num-tokens=?*)
            NUM_TOKENS=${1#*=}
            ;;
        --num-tokens=)
        printf 'ERROR: "--num-tokens" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --spark-shuffle-service-port)
            if [ -n "$2" ]; then
                SPARK_SHUFFLE_SERVICE_PORT=$2;
                shift 2;
                continue
            else
                printf 'ERROR: "--spark-shuffle-service-port" requires a non-empty option argument.\n' >&2
                exit 1
            fi
            ;;
        --spark-shuffle-service-port=?*)
            SPARK_SHUFFLE_SERVICE_PORT=${1#*=}
            ;;
        --spark-shuffle-service-port=)
        printf 'ERROR: "--spark-shuffle-service-port" requires a non-empty option argument.\n' >&2
        exit 1
        ;;

        --dsefs)
            ENABLE_DSEFS=1
            shift;
            continue
            ;;

        --)              # End of all options.
            shift
            break
            ;;
        -?*)
            printf 'ERROR: Unknown option: %s\n' "$1" >&2
            exit 1
            ;;
        *)
            break
    esac

    shift
done

if [ -z $NODEID ]; then
    show_help
    echo

    printf "No node ID specified. Please specify one via -n|--node-id\n"

    exit 1;
fi

# remove 'dse-' prefix, just to add it again. This means the script
# will accept node1, or dse-node1, as NODEID
NODEID="dse-`echo $NODEID | sed 's/^dse-//g'`"

# validate that NODEID contains no glob chars
if [[ "$NODEID" =~ [*?] ]] ; then
    echo "The name of the new node contains invalid characters (* or ?). Please try again."
    exit 1
fi

# this only works with debian/rpm packages for the moment
if [ -z $ROOT_CONFIG ]; then
    ROOT_CONFIG=/etc
fi

SERVER_CONFIG=$ROOT_CONFIG/dse/serverconfig

if [ -z $DSE_HOME ]; then
    DSE_HOME=/usr/share/dse
fi

if [ -z $DSE_BIN ]; then
    DSE_BIN=/usr/bin
fi

if [ -f $SERVER_CONFIG/$NODEID ]; then
    echo "Node ${NODEID} exists. Not overwriting."
    exit 0;
fi

if [ -e /etc/$NODEID ]; then
    echo "/etc/${NODEID} exists. Not adding node as this could break other packages."
    exit 1;
fi

if [ -z $BASE_DIRECTORY ]; then
    BASE_DIRECTORY=/var/lib/$NODEID
fi

if [ -z $DATA_DIRECTORY ]; then
    DATA_DIRECTORY=$BASE_DIRECTORY/data
fi

if [ -z $HINTS_DIRECTORY ]; then
    HINTS_DIRECTORY=$BASE_DIRECTORY/hints
fi

if [ -z $COMMIT_DIRECTORY ]; then
    COMMIT_DIRECTORY=$BASE_DIRECTORY/commitlog
fi

if [ -z $CACHES_DIRECTORY ]; then
    CACHES_DIRECTORY=$BASE_DIRECTORY/saved_caches
fi

if [ -z $ROOT_LOGS ]; then
    ROOT_LOGS=/var/log/$NODEID
fi

if [ -z $TOMCAT_LOGS ]; then
    TOMCAT_LOGS=$ROOT_LOGS/tomcat
fi

if [ -z $HADOOP_LOGS ]; then
    HADOOP_LOGS=$ROOT_LOGS/hadoop
fi

if [ -z $HIVE_LOG_ROOT ]; then
    HIVE_LOG_ROOT=$ROOT_LOGS
fi

if [ -z $PIG_LOG_ROOT ]; then
    PIG_LOG_ROOT=$ROOT_LOGS
fi

if [ -z $MAHOUT_LOGS ]; then
    MAHOUT_LOGS=$ROOT_LOGS/mahout
fi

if [ -z $SPARK_WORKER_DIR ]; then
    SPARK_WORKER_DIR=$BASE_DIRECTORY/spark/worker
fi

if [ -z $SPARK_LOCAL_DIRS ]; then
    SPARK_LOCAL_DIRS=$BASE_DIRECTORY/spark/rdd
fi

if [ -z $SPARK_WORKER_LOG_DIR ]; then
    SPARK_WORKER_LOG_DIR=/var/log/$NODEID/spark/worker
fi

if [ -z $UNIX_USERNAME ]; then
    UNIX_USERNAME="cassandra"
fi

if [ -z $UNIX_GROUP ]; then
    UNIX_GROUP="cassandra"
fi

if [ -z $RACK ] && [ ! -z $DC ]; then
    echo "Both --rack and --dc must be set."
    exit 1;
fi

if [ ! -z $RACK ] && [ -z $DC ]; then
    echo "Both --rack and --dc must be set."
    exit 1;
fi

RUNNING_AS=`id -un`
if [ ! "$RUNNING_AS" = "root" ] && [ ! "$RUNNING_AS" = $UNIX_USERNAME ]; then
    echo "Warning: you're running as $RUNNING_AS. It's possible that some operations will fail due to incorrect permissions."
fi


printf "Installing and configuring from %s/templates/\n\n" $DSE_HOME

# create directories for holding the node's data, logs, etc.
make_dir_and_set_perms()
{
    local dir=$1;
    mkdir -p $dir
    chmod o-rwx $dir
    chown $UNIX_USERNAME:$UNIX_GROUP $dir
}

create_dirs()
{
    local nodeid=$1

    make_dir_and_set_perms $DATA_DIRECTORY
    make_dir_and_set_perms $COMMIT_DIRECTORY
    make_dir_and_set_perms $CACHES_DIRECTORY
    make_dir_and_set_perms $HINTS_DIRECTORY
    # needed to make parent owned by the right user
    make_dir_and_set_perms $SPARK_LOCAL_DIRS/../
    make_dir_and_set_perms $SPARK_LOCAL_DIRS

    # needed to make parent owned by the right user
    make_dir_and_set_perms $SPARK_WORKER_DIR/../
    make_dir_and_set_perms $SPARK_WORKER_DIR

    make_dir_and_set_perms $ROOT_LOGS
    make_dir_and_set_perms $SPARK_WORKER_LOG_DIR

    make_dir_and_set_perms $ROOT_CONFIG/$nodeid

    chown -R $UNIX_USERNAME:$UNIX_GROUP $ROOT_CONFIG/$nodeid
}

# copy the relevant sections of the config for a service
copy_config()
{
    local nodeid="$1"
    local base_dir=$2

    local src="$base_dir/templates/$service"
    local dst="$ROOT_CONFIG/$nodeid"

    mkdir -p $dst

    for s in `ls $base_dir/templates`; do
        local src="$base_dir/templates/$s"
        if [ -d $src ] && [ ! $s = "dse" ]; then
            cp -r $src $dst
        fi
    done

    chown -R $UNIX_USERNAME:$UNIX_GROUP "$ROOT_CONFIG/$nodeid"
}

# DSE is special
setup_dse()
{
    local nodeid="$1"
    local base_dir=$2

    local src="$base_dir/templates/dse"
    local dst="$ROOT_CONFIG/$nodeid/"

    local node_defaults="/etc/default/$nodeid"

    mkdir -p $dst

    cp $src/dse.yaml $dst
    cp $src/dse-env.sh $dst
    cp $src/dserc-env.sh $dst
    cp $src/byoh-env.sh $dst
    cp $src/dse.default /etc/default/$nodeid

    chown -R $UNIX_USERNAME:$UNIX_GROUP "$ROOT_CONFIG/$nodeid"
    chown $UNIX_USERNAME:$UNIX_GROUP $node_defaults

    replace "OUTPUT_FILE=\"/var/log/cassandra/output.log\"" "OUTPUT_FILE=$ROOT_LOGS/output.log" $node_defaults

    replace "CASSANDRA_CONF=/etc/dse/cassandra" "CASSANDRA_CONF=$ROOT_CONFIG/$nodeid/cassandra" $node_defaults

    replace "SPARK_CONF_DIR=/etc/dse/spark" "SPARK_CONF_DIR=$ROOT_CONFIG/$nodeid/spark" $node_defaults

    replace "GRAPH_CONF_DIR=/etc/dse/graph" "GRAPH_CONF_DIR=$ROOT_CONFIG/$nodeid/graph" $node_defaults

    if [ ! -z $SOLR_ENABLED ]; then
        replace "SOLR_ENABLED=0" "SOLR_ENABLED=$SOLR_ENABLED" $node_defaults
    fi

    if [ ! -z $SPARK_ENABLED ]; then
        replace "SPARK_ENABLED=0" "SPARK_ENABLED=$SPARK_ENABLED" $node_defaults
    fi

    if [ ! -z $HADOOP_ENABLED ]; then
        replace "HADOOP_ENABLED=0" "HADOOP_ENABLED=$HADOOP_ENABLED" $node_defaults
    fi

    if [ ! -z $GRAPH_ENABLED ]; then
        replace "GRAPH_ENABLED=0" "GRAPH_ENABLED=$GRAPH_ENABLED" $node_defaults
    fi

    if [ ! -z $CFS_ENABLED ]; then
        replace "CFS_ENABLED=0" "CFS_ENABLED=$CFS_ENABLED" $node_defaults
    fi
}

replace()
{
    local original="$1";
    local replacement="$2";
    local file="$3";

    local regexp="s|$original|$replacement|g";

    sed -i -- "$regexp" "$file"
}

enable_dsefs()
{
    local file="$1";

    sed -i -- '/dsefs_options:/,/^\s*$/ s/enabled: false/enabled: true/' $file
}

# adjust the cassandra config for a node
tweak_cassandra_config()
{
    local nodeid=$1
    local env="$ROOT_CONFIG/$nodeid/cassandra/cassandra-env.sh"
    local conf="$ROOT_CONFIG/$nodeid/cassandra/cassandra.yaml"
    local logs="$ROOT_CONFIG/$nodeid/cassandra/logback.xml"
    local tomcat_server_xml="$ROOT_CONFIG/$nodeid/tomcat/conf/server.xml"

    # Set the cluster name
    if [ ! -z $CLUSTER ]; then
        echo "      - Setting the cluster name."
        replace "cluster_name: 'Test Cluster'" "cluster_name: $CLUSTER" $conf
    fi

    # Set the JMX port
    if [ ! -z $JMX_PORT ]; then
        printf "      - Setting up JMX port\n"
        replace "JMX_PORT=\"7199\"" "JMX_PORT=\"$JMX_PORT\"" $env
    fi

    # Set the number of CPUs to use
    if [ ! -z $CPUS ]; then
        printf "      - Using $CPUS cpus\n"
        echo "JVM_OPTS=\"\$JVM_OPTS -Dcassandra.available_processors=$CPUS\"" >> $env
    fi

    if [ ! -z $MAX_HEAP_SIZE ]; then
        printf "      - Setting up max heap size\n"
        if [[ "$MAX_HEAP_SIZE" =~ ^[0-9]+$ ]]; then
            MAX_HEAP_SIZE="${MAX_HEAP_SIZE}M"
        fi
        replace "#MAX_HEAP_SIZE=\"4G\"" "MAX_HEAP_SIZE=\"${MAX_HEAP_SIZE}\"" $env
    fi

    # Set the commitlog directory, and various other directories
    printf "      - Setting up directories\n"
    replace "/var/lib/cassandra/commitlog" "$COMMIT_DIRECTORY" $conf

    # data_file_directories
    replace "/var/lib/cassandra/data" "$DATA_DIRECTORY" $conf

    # hints
    replace "/var/lib/cassandra/hints" "$HINTS_DIRECTORY" $conf

    # saved_caches_directory
    replace "/var/lib/cassandra/saved_caches" "$CACHES_DIRECTORY" $conf

    if [ ! -z $LISTEN_ADDRESS ]; then
        replace "listen_address: localhost" "listen_address: $LISTEN_ADDRESS" $conf
    fi

    if [ ! -z $RPC_ADDRESS ]; then
        replace "rpc_address: localhost" "rpc_address: $RPC_ADDRESS" $conf
    fi

    if [ ! -z $SEEDS ]; then
        replace "- seeds: \"127.0.0.1\"" "- seeds: \"$SEEDS\"" $conf
    fi

    if [ ! -z $RACK ]; then
        local rackdc_properties="$ROOT_CONFIG/$nodeid/cassandra/cassandra-rackdc.properties"
        # Set the snitch to GPFS
        replace "endpoint_snitch: com.datastax.bdp.snitch.DseSimpleSnitch" "endpoint_snitch: GossipingPropertyFileSnitch" $conf

        # set the rack
        replace "rack=rack1" "rack=$RACK" $rackdc_properties

        # set the dc
        replace "dc=dc1" "dc=$DC" $rackdc_properties
        echo "NOTE: Please check the DC/Rack placements of the other nodes in this server for mismatches."
    fi

    if [ ! -z $NUM_TOKENS ]; then
        replace "# num_tokens: 128" "num_tokens: $NUM_TOKENS" $conf
    fi
}

tweak_spark_config()
{
    local nodeid=$1
    local spark_env="$ROOT_CONFIG/$nodeid/spark/spark-env.sh"
    local spark_defaults="$ROOT_CONFIG/$nodeid/spark/spark-defaults.conf"
    local dse_yaml="$ROOT_CONFIG/$nodeid/dse.yaml"

    # SPARK_WORKER_DIR
    replace "export SPARK_WORKER_DIR=\"/var/lib/spark/worker\"" "export SPARK_WORKER_DIR=$SPARK_WORKER_DIR" $spark_env

    # SPARK_LOCAL_DIRS
    replace "export SPARK_LOCAL_DIRS=\"/var/lib/spark/rdd\"" "export SPARK_LOCAL_DIRS=$SPARK_LOCAL_DIRS" $spark_env

    # SPARK_WORKER_LOG_DIR
    replace "export SPARK_WORKER_LOG_DIR=\"/var/log/spark/worker\"" "export SPARK_WORKER_LOG_DIR=$SPARK_WORKER_LOG_DIR" $spark_env

    # SPARK_WORKER_CORES
    if [ ! -z $SPARK_WORKER_CORES ]; then
        replace "# export SPARK_WORKER_CORES=4" "export SPARK_WORKER_CORES=$SPARK_WORKER_CORES" $spark_env
    fi
    # SPARK_WORKER_MEMORY
    if [ ! -z $SPARK_WORKER_MEMORY ]; then
        replace "# export SPARK_WORKER_MEMORY=2048m" "export SPARK_WORKER_MEMORY=$SPARK_WORKER_MEMORY" $spark_env
    fi

    # Hadoop task tracker cores
    if [ ! -z $HADOOP_TT_CORES ]; then
        replace "# task_tracker_cores: 2" "task_tracker_cores: $HADOOP_TT_CORES" $dse_yaml
    fi

    # Hadoop task tracker memory
    if [ ! -z $HADOOP_TT_MEMORY ]; then
        replace "# task_tracker_memory: 4g" "task_tracker_memory: $HADOOP_TT_MEMORY" $dse_yaml
    fi

    # Spark shuffle service port
    if [ ! -z $SPARK_SHUFFLE_SERVICE_PORT ]; then
        replace "spark.shuffle.service.port 7737" "spark.shuffle.service.port $SPARK_SHUFFLE_SERVICE_PORT" $spark_defaults
    else
        echo "Warning: Spark shuffle service port not set. Spark nodes will use the default binding options."
    fi

    if [ ! -z $ENABLE_DSEFS ]; then
        enable_dsefs $dse_yaml
        replace "/var/lib/dsefs" "$DATA_DIRECTORY/dsefs" $dse_yaml
        replace "/var/lib/dsefs/data" "$DATA_DIRECTORY/dsefs/data" $dse_yaml
    fi
}

tweak_graph_config()
{
    local nodeid=$1
    local dse_yaml="$ROOT_CONFIG/$nodeid/dse.yaml"
    local gremlin_console_conf="$ROOT_CONFIG/$nodeid/graph/gremlin-console/conf/remote.yaml"

    if [ ! -z $LISTEN_ADDRESS ]; then
        replace "^    host: localhost$" "    host: $LISTEN_ADDRESS" $dse_yaml
        replace "hosts: \[localhost\]" "hosts: \[$LISTEN_ADDRESS\]" $gremlin_console_conf
    fi
}

setup_resources()
{
    printf "  - Copying configs\n"
    copy_config $1 $2 "cassandra"
    copy_config $1 $2 "hadoop"
    copy_config $1 $2 "pig"
    copy_config $1 $2 "hive"
    copy_config $1 $2 "sqoop"
    copy_config $1 $2 "mahout"
    copy_config $1 $2 "spark"
    copy_config $1 $2 "tomcat"
    copy_config $1 $2 "byoh"
    copy_config $1 $2 "solr"
    copy_config $1 $2 "hadoop2-client"
    copy_config $1 $2 "graph/gremlin-console"

    setup_dse $1 $2
}

create_initd_script()
{
    local nodeid="$1"
    local node_init=$ROOT_CONFIG/$nodeid/$nodeid.init

    cp $DSE_HOME/templates/dse.init $node_init
    chmod +x $node_init

    replace "DESC=\"DSE daemon\"" "DESC=\"DSE daemons ($nodeid)\"" $node_init
    replace "NAME=\"dse\"" "NAME=$nodeid" $node_init
    replace "CASSANDRA_USER=\"cassandra\"" "CASSANDRA_USER=$UNIX_USERNAME" $node_init

    replace "OUTPUT_FILE=\"/var/log/cassandra/output.log\"" "OUTPUT_FILE=$ROOT_LOGS/output.log" $node_init

    mkdir -p /var/run/$nodeid
    chown $UNIX_USERNAME /var/run/$nodeid

    ln -s $node_init /etc/init.d/$nodeid
    update-rc.d -f $nodeid defaults > /dev/null 2>&1
}

create_node_config()
{
    local nodeid=$1;
    local node_config="$SERVER_CONFIG/$nodeid"

    cp $DSE_HOME/templates/node_config $node_config

    replace "NODEID=\"REPLACEME\"" "NODEID=$nodeid" $node_config
    replace "NODE_CONFIG=\"REPLACEME\"" "NODE_CONFIG=$ROOT_CONFIG/$nodeid" $node_config

    replace "DSE_HOME=\"REPLACEME\"" "DSE_HOME=$DSE_HOME" $node_config
    replace "DSE_ENV=\"REPLACEME\"" "DSE_ENV=\$NODE_CONFIG/dse-env.sh" $node_config
    replace "DSE_LOG_ROOT=\"REPLACEME\"" "DSE_LOG_ROOT=$ROOT_LOGS" $node_config
    replace "CASSANDRA_LOG_DIR=\"REPLACEME\"" "CASSANDRA_LOG_DIR=$ROOT_LOGS" $node_config
    replace "TOMCAT_LOGS=\"REPLACEME\"" "TOMCAT_LOGS=$TOMCAT_LOGS" $node_config
    replace "HADOOP_LOG_DIR=\"REPLACEME\"" "HADOOP_LOG_DIR=$HADOOP_LOGS" $node_config
    replace "HIVE_LOG_ROOT=\"REPLACEME\"" "HIVE_LOG_ROOT=$HIVE_LOG_ROOT" $node_config
    replace "SPARK_LOG_DIR=\"REPLACEME\"" "SPARK_LOG_DIR=$SPARK_WORKER_LOG_DIR" $node_config
    replace "PIG_LOG_ROOT=\"REPLACEME\"" "PIG_LOG_ROOT=$PIG_LOG_ROOT" $node_config
    replace "MAHOUT_LOG_DIR=\"REPLACEME\"" "MAHOUT_LOG_DIR=$MAHOUT_LOGS" $node_config
}

setup_node()
{
    local nodeid=$1

    printf "+ Setting up node %s...\n" $nodeid
    create_dirs $nodeid
    setup_resources $nodeid $DSE_HOME

    tweak_cassandra_config $nodeid
    tweak_spark_config $nodeid
    tweak_graph_config $nodeid

    # see https://unix.stackexchange.com/a/164092 (and the question itself)
    # for more details
    # In our case, we check for DSE's init script, if it's present we go along
    # with it
    if [ -f /etc/init.d/dse ]; then
        create_initd_script $nodeid
    fi

    create_node_config $nodeid
}

setup_node $NODEID

printf "Done.\n"
